{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyFbiPnzt8pB"
   },
   "source": [
    "# Week 2, Day 3 (Guided Project with Pytorch on Image Classification)\n",
    "\n",
    "We recommand you to try this notebook on colab by clicking the link below.<br>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
    "\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "- [Import Necessary Libraries](#import)\n",
    "- [Main Components](#components)\n",
    "- [Dataset](#dataset)\n",
    "    - [Dataset and DataLoader](#dataloader)\n",
    "- [Model](#model)\n",
    "- [Inspect](#inspect)\n",
    "- [Training](#training)\n",
    "- [Saving and Loading](#saving)\n",
    "- [Testing](#testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgJGtoDugBEx"
   },
   "source": [
    "<a id=import></a>\n",
    "## Import Necessary Libraries\n",
    "\n",
    "Pytorch is the open source machine learning framework that we can use for research to production.\n",
    "\n",
    "If you would like to study the tutorials from the offical [pytorch](https://pytorch.org/) website, please visit [this link](https://pytorch.org/tutorials/). The source code for the entire pytorch framework can be found [here](https://github.com/pytorch).\n",
    "\n",
    "> SideNote: Pytorch separates different tasks into different modules. e.g., there is a package called **torchaudio** that focus only on audio alone.\n",
    "\n",
    "\n",
    "Today we will use `torchvision`, which is the package inside torch, that focus on vision tasks. For example, for data augmentation, torchvision provides the function called **transforms**. And if you would like to use transfer learning, torchvision also provides some of the state of the art pretrained models.\n",
    "\n",
    "The `torch.nn` contains the basic building blocks that we need to construct our model. For example, if we need to construct a **Convolutional Layer**, we can call the function `torch.nn.Conv2d()` to construct that layer.\n",
    "\n",
    "And if we want linear layer that perform the equation of $y = W^TX + b$, we can call the function `torch.nn.Linear()`.\n",
    "\n",
    "If you would like to know more about `torch.nn` library, please visit this [link](https://pytorch.org/docs/stable/nn.html) for more information. Also, this [post](https://pytorch.org/tutorials/beginner/nn_tutorial.html) provides better understanding of `torch.nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5270,
     "status": "ok",
     "timestamp": 1608804619957,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "wejqfkizlvnD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uDlHmO-pCWH"
   },
   "source": [
    "<a id=components></a>\n",
    "## Main Components\n",
    "\n",
    "Let's preview what will be built in this notebook.\n",
    "\n",
    " 1. Dataset\n",
    "     - CIFAR10\n",
    " 2. Model Architecture\n",
    "     - Simple Model\n",
    " 3. Loss (Update model)\n",
    "     - Categorical Cross Entropy\n",
    " 4. Optimizer (Regularizer)\n",
    "     - Adam Optimizer\n",
    " 5. Metrics (Visual for User)\n",
    "     - Accuracy\n",
    " 6. Training Model\n",
    " 7. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmfg8ySjn0D1"
   },
   "source": [
    "If you would like to change from CPU to GPU,\n",
    "select the `Runtime --> Change Runtime type` and select `GPU`.\n",
    "\n",
    "After done selecting, we can check whether we are running GPU or CPU by using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5203,
     "status": "ok",
     "timestamp": 1608804619959,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "SSDu6fd4t8pW",
    "outputId": "2a1e92f3-0b90-446d-a09c-2a74cf50124c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5126,
     "status": "ok",
     "timestamp": 1608804619960,
     "user": {
      "displayName": "Aung Paing",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtNjXDzCg0wqagLdTo1q7N18g1ys6GIxknFcJN=s64",
      "userId": "03851297839959231119"
     },
     "user_tz": -390
    },
    "id": "xfhNywLHWTco",
    "outputId": "c5880e0a-c9bf-49f6-c7dd-b04ce6aceacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP_JtqqTt8pY"
   },
   "source": [
    "If the output show `cuda:0`, then the `GPU` is in used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Bvkx_6YoXM0"
   },
   "source": [
    "<a id=dataset></a>\n",
    "## Dataset\n",
    "\n",
    "In this notebook, we will use `CIFAR10` dataset for classification. \n",
    "\n",
    "[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) is a public classification dataset, which consists of 60,000 32 * 32 color images in 10 classes. There are 50,000 training images and 10,000 testing images. \n",
    "\n",
    "In the following cell, we will do data augmentation for the dataset. When doing **trasnforms** on the test dataset, we only *normalize* the input, that is because we do not need to augment(change) our test dataset to see the result.\n",
    "\n",
    "Tensor is just like Numpy's ndarray, except that it can do calculations on GPU, and is modified to fit the training neural network procedure.\n",
    "\n",
    "Normalization equation:\n",
    "\n",
    "$X = \\frac{X-\\mu}{\\sigma}$.\n",
    "\n",
    "There are many augmentation methods available, if you would like to know more, please visit this [post](https://pytorch.org/docs/stable/torchvision/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "7cfa2a512265471981012eb684dfc822",
      "8a9f0f12963e463c88b31e3233dce586",
      "cd4143d454ba4a579bcc2d1e9b344ca0",
      "3b29eb7ec4c74bc0ba997cc1a1170b4f",
      "1f28769831294a4fb407b3bd3fa4eb0c",
      "c726b1027a354e66a06fb3a04a5f7ed1",
      "973499ff2d344a9f8ab8db72b5655051",
      "0bebddaf1d914fe9b0c9afecbc18cddb"
     ]
    },
    "id": "QOOHZDMPo1UV",
    "outputId": "80e75a12-7a5b-4e16-a764-45b768314020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfa2a512265471981012eb684dfc822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "# The class names for CIFAR 10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "# Image Normalization, Data Augmentation\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                transforms.RandomRotation(30)\n",
    "                                ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "        './data', train=True,\n",
    "        transform=transform, download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "        './data', train=False,\n",
    "        transform=test_transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afcYs8kvvkEu"
   },
   "source": [
    "> Note: Pytorch stores the image dataset in the following format:\n",
    "`batch_size * dim * height * width (B*D*H*W)`.\n",
    "\n",
    "`batch_size` means number of images for one training iteration.\n",
    "\n",
    "`dim` means the image dimension. Conventionally, color images have the dimension of *3* and gray scale images have the dimension of *1*.\n",
    "\n",
    "The label value for the dataset ranges from `0~9`. e.g., if the label is *7*, then, it would be **class_names[7] = horse**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU-e7QXcx1Jp"
   },
   "source": [
    "<a id=dataloader></a>\n",
    "### Dataset and DataLoader\n",
    "\n",
    "Dataset play the huge role in machine learning and deep learning. For the computer vision task, we could do data augmentation to get the effect of regulating the model, avoid being overfitting. And when training, the augmented dataset need to be fetched by the data loader. The main duty of the dataloader is to prepare dataset before feeding to the neural network.\n",
    "\n",
    "Dataloader object is a generator object, which can be accessed through iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fl5zipzpsfMO"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, 32, shuffle=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, 8, shuffle=True,\n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQU-uJSh2xu4"
   },
   "source": [
    "<a id=model></a>\n",
    "## Model\n",
    "\n",
    "Let's build a simple Convolutional Neural Network.\n",
    "\n",
    "When flattening the network, we can use the equation:\n",
    "\n",
    "$O = \\frac{W-K+2P}{S} + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PAj_NegsfOi"
   },
   "outputs": [],
   "source": [
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 256, 3)\n",
    "        self.conv3 = nn.Conv2d(256, 256, 3)\n",
    "        self.conv4 = nn.Conv2d(256, 128, 3)\n",
    "        self.fc1 = nn.Linear(24*24*128, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(-1, 24*24*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYI-5pmqsfQ8"
   },
   "outputs": [],
   "source": [
    "model = SampleModel()\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf9G98RAZAF2"
   },
   "source": [
    "<a id=inspect></a>\n",
    "## Let's Inspect our model.\n",
    "\n",
    "We can get the weights of the model by two methods: `model.parameters()` and `model.state_dict()`.\n",
    "\n",
    "`model.parameters()` contain the value of the weight. For example, in Conv2d layer, it will be the kernel value.\n",
    "\n",
    "`model.state_dict()` is a dictionary object which have key as the parameter name and value of model.parameters()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtk2qj7HY_HI"
   },
   "outputs": [],
   "source": [
    "print(type(model.state_dict()))\n",
    "print(type(model.parameters()))\n",
    "print()\n",
    "\n",
    "for para in model.parameters():\n",
    "    print(para.size())\n",
    "    break\n",
    "\n",
    "for stats in model.state_dict():\n",
    "    print(f\"Key : {stats}, Value : {model.state_dict()[stats].size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-inEks5rbWhp"
   },
   "outputs": [],
   "source": [
    "# We could also write the above with key manually provided.\n",
    "first_conv = model.state_dict()['conv1.weight']\n",
    "print(first_conv.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S24XhmrXadGN"
   },
   "outputs": [],
   "source": [
    "# Helper function for convolution in 3D image\n",
    "def conv2d_cv2(img, filter):\n",
    "    h, w, d = img.shape\n",
    "    filtered = np.zeros_like(img)\n",
    "    for i in range(3):\n",
    "        img_ = img[:,:,i]\n",
    "        filter_ = filter[:,:,i]\n",
    "        result = cv2.filter2D(img_, -1, filter_)\n",
    "        filtered[:,:,i] = result\n",
    "\n",
    "    filtered = filtered.mean(axis=2)\n",
    "    return filtered\n",
    "\n",
    "# Helper function\n",
    "def visualize_filters(model=model):\n",
    "    '''\n",
    "    Visualize Convolutional layer (Weight of model).\n",
    "    '''\n",
    "    first_conv = model.state_dict()['conv1.weight']\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, filter in enumerate(first_conv):\n",
    "        filter = filter.cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "        filter += np.abs(np.min(filter))\n",
    "        filter /= np.max(filter)\n",
    "        plt.subplot(8, 8, i+1)\n",
    "        plt.imshow(filter)\n",
    "        # print(np.max(filter), np.min(filter))\n",
    "        # break\n",
    "    plt.show()\n",
    "\n",
    "# Helper function to visualize first layer output\n",
    "def visualize_model(model=model, img_ind = 10):\n",
    "    '''\n",
    "    Visualize model for better understanding.\n",
    "    Args:\n",
    "        model -> model trained or not trained.\n",
    "    '''\n",
    "    image = train_dataset[img_ind][0].cpu().numpy().transpose(1, 2, 0)\n",
    "    first_conv = model.state_dict()['conv1.weight']\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, filter in enumerate(first_conv):\n",
    "        filter = filter.cpu().numpy().transpose(1, 2, 0)\n",
    "        filtered = conv2d_cv2(image, filter)\n",
    "        plt.subplot(8, 8, i+1)\n",
    "        plt.imshow(filtered, cmap='gray')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_filters(model)\n",
    "visualize_model(model, img_ind=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=training></a>\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9y8EErl8Vqf"
   },
   "outputs": [],
   "source": [
    "# Train and plot model performance\n",
    "EPOCH = 10\n",
    "\n",
    "loss_log = []\n",
    "acc_log = []\n",
    "x_coor = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_epoch = 0\n",
    "    total_imgs = 0\n",
    "    correct_epoch = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        # Get Image data and Label data\n",
    "        imgs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Clear out the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # Forward Propagation Start\n",
    "        # Predict from Input : B * 10\n",
    "        predicts = model(imgs)\n",
    "\n",
    "        # Calculate Loss from batch : 1\n",
    "        loss = criterion(predicts, labels)\n",
    "        # Forward Propagation End\n",
    "\n",
    "\n",
    "        # Backward Propagation Start\n",
    "        # Calculate Gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Model parameters with optimizer : Adam or SGD\n",
    "        optimizer.step()\n",
    "        # Backward Propagation End\n",
    "\n",
    "\n",
    "        # Add to Epoch Loss\n",
    "        loss_epoch += loss.item()\n",
    "        # Total Number of images in one batch\n",
    "        total_imgs += len(imgs)\n",
    "        # Count the total number of correct prediction\n",
    "        correct_batch = (torch.argmax(predicts, 1)==labels).sum().item()\n",
    "        correct_epoch += correct_batch\n",
    "        acc_batch = correct_batch/ len(imgs)\n",
    "\n",
    "        # Adding to tensorboard\n",
    "        x_coor.append((i*len(imgs))+(epoch*len(train_dataset)))\n",
    "        loss_log.append(loss.item())\n",
    "        acc_log.append(acc_batch)\n",
    "\n",
    "\n",
    "    acc_epoch = (correct_epoch/total_imgs)*100\n",
    "    loss_epoch = loss_epoch/total_imgs\n",
    "    print(f\"EPOCH : {epoch+1}, Acc : {acc_epoch:.2f}, Loss : {loss_epoch:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_noCBg2hAgJ"
   },
   "outputs": [],
   "source": [
    "# Let's visualize our model filter again\n",
    "visualize_filters(model)\n",
    "visualize_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJTs4WiGt8pl"
   },
   "source": [
    "<a id=saving></a>\n",
    "## Saving and Loading\n",
    "\n",
    "When saving our model, we often save the model `state_dict`. The saved model file contain the values for the weight and the variable name indicating that weight.\n",
    "\n",
    "So, if we try to reload the model next time, the file will automatically look for the same variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf6RjLLV6T5P"
   },
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "ckpt_path = 'checkpoint.pt'\n",
    "\n",
    "# State Difference between model.parameters and model.state_dict()\n",
    "torch.save(model.state_dict(), ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thAarnq5azZQ"
   },
   "outputs": [],
   "source": [
    "saved_model = SampleModel().to(device)\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "saved_model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10nZFQNQw9U1"
   },
   "source": [
    "<a id=testing></a>\n",
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9tiqgIiiqbr"
   },
   "outputs": [],
   "source": [
    "# Test Dataset accuracy:\n",
    "num_total = 0\n",
    "num_correct = 0\n",
    "\n",
    "# Deactivate Drop out and Batch-Normalization layers.\n",
    "model.eval()\n",
    "\n",
    "# Do not store gradient info in forward propagation.\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        image, label = data\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        predict = model(image)\n",
    "        predict_class = torch.argmax(predict, dim=1)\n",
    "        correct = (predict_class == label)\n",
    "\n",
    "\n",
    "        num_correct += correct.sum().item()\n",
    "        num_total += len(image)\n",
    "        \n",
    "print(num_correct, num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSVOWVrviqh5"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy : \", num_correct/num_total)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2020-12-21-week2-day3_optional_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0bebddaf1d914fe9b0c9afecbc18cddb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f28769831294a4fb407b3bd3fa4eb0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3b29eb7ec4c74bc0ba997cc1a1170b4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bebddaf1d914fe9b0c9afecbc18cddb",
      "placeholder": "​",
      "style": "IPY_MODEL_973499ff2d344a9f8ab8db72b5655051",
      "value": " 162193408/170498071 [00:01&lt;00:00, 95393129.21it/s]"
     }
    },
    "7cfa2a512265471981012eb684dfc822": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd4143d454ba4a579bcc2d1e9b344ca0",
       "IPY_MODEL_3b29eb7ec4c74bc0ba997cc1a1170b4f"
      ],
      "layout": "IPY_MODEL_8a9f0f12963e463c88b31e3233dce586"
     }
    },
    "8a9f0f12963e463c88b31e3233dce586": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "973499ff2d344a9f8ab8db72b5655051": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c726b1027a354e66a06fb3a04a5f7ed1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd4143d454ba4a579bcc2d1e9b344ca0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": " 95%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c726b1027a354e66a06fb3a04a5f7ed1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1f28769831294a4fb407b3bd3fa4eb0c",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
